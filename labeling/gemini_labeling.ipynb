{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comments = pd.read_csv(\"../data/full_comments.csv\")\n",
    "examples = full_comments.loc[full_comments['true_label'].notna(), [\"comment_text\", \"true_label\"]].head(10)\n",
    "negative_examples = pd.read_excel(\"negative_examples.xlsx\", names=['comment_text', 'original_label', 'corrected_label'])\n",
    "\n",
    "examples = examples.rename(columns={\"comment_text\": \"comment\", \"true_label\": \"label\"})\n",
    "\n",
    "examples_json = json.dumps(examples.to_dict(orient=\"records\"), indent=2)\n",
    "negative_examples_json = json.dumps(negative_examples.to_dict(orient=\"records\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = full_comments.sample(30000, random_state=10)\n",
    "subset['label'] = subset['true_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_COLUMN = \"comment_text\"\n",
    "OUTPUT_CSV = \"gemini_comments_labeled.csv\"\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('GEMINI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL = \"models/gemini-3-flash-preview\"\n",
    "BATCH_SIZE = 50\n",
    "MAX_REQUESTS_PER_JOB = 100\n",
    "JOB_NAMES_FILE = \"gemini_job_names.json\"\n",
    "\n",
    "# ============================================================\n",
    "# SYSTEM PROMPT\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT = f\"\"\"You are a comment classifier. You will be given a batch of comments, each with an ID number. \n",
    "Classify each comment into exactly ONE of these five categories:\n",
    "\n",
    "**Argumentative**\n",
    "- Makes specific claims, predictions, or assertions supported by reasoning\n",
    "- Uses evidence, anecdotes, or scenarios to build a case\n",
    "- The key distinction from Opinion: there's an attempt to *persuade* or *explain why*, not just state a position\n",
    "\n",
    "**Informational**\n",
    "- Shares facts, data, links, or context relevant to the discussion\n",
    "- Low emotional affect — the comment is trying to *inform*, not convince or react\n",
    "- Includes answering another commenter's question with factual content\n",
    "- The key distinction from Argumentative: presenting information without advocating for a position\n",
    "\n",
    "**Opinion**\n",
    "- States a value judgment, stance, or take without substantial reasoning\n",
    "- \"This is good/bad/wrong/overrated\" — the comment *asserts* but doesn't *argue*\n",
    "- The key distinction from Argumentative: no real attempt to persuade or support the claim\n",
    "- The key distinction from Expressive: the comment is making a point, not just reacting\n",
    "\n",
    "**Expressive**\n",
    "- Emotional reactions, sarcasm, jokes, venting, exclamations\n",
    "- The comment is primarily *expressing feeling* rather than making a point\n",
    "- Includes performative agreement/disagreement (\"THIS,\" \"lol exactly,\" \"what a joke\")\n",
    "- The key distinction from Opinion: no identifiable stance being taken, just affect\n",
    "\n",
    "**Neutral**\n",
    "- Clarifying or rhetorical questions, meta-commentary, off-topic remarks\n",
    "- Comments that don't clearly fit the other four categories\n",
    "- Includes simple factual questions directed at other commenters\n",
    "\n",
    "**Correctly labeled examples** — these demonstrate the correct label for each comment:\n",
    "{examples_json}\n",
    "\n",
    "**Incorrectly labeled examples** — these were originally mislabeled. The \"original_label\" is the wrong label that was assigned, and the \"corrected_label\" is what the label should have been. Use these to understand common mistakes to avoid:\n",
    "{negative_examples_json}\n",
    "\n",
    "Respond with ONLY a valid JSON array where each element has \"id\", \"label\" keys and a confidence indicator where \n",
    "0 is not confident in the chosen label and 1 is confident in the chosen label.\n",
    "Example: [{{\"id\": 0, \"label\": \"Argumentative\", \"confidence\": 1}}, {{\"id\": 1, \"label\": \"Expressive\", \"confidence\": 0}}]\n",
    "\n",
    "Do not include any text outside the JSON array. No explanations, no markdown.\"\"\"\n",
    "\n",
    "VALID_LABELS = {\"Argumentative\", \"Informational\", \"Opinion\", \"Expressive\", \"Neutral\"}\n",
    "\n",
    "\n",
    "def format_batch(comments):\n",
    "    lines = []\n",
    "    for idx, comment in comments:\n",
    "        truncated = comment[:1500] if len(comment) > 1500 else comment\n",
    "        lines.append(f\"[{idx}] {truncated}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def parse_response(response_text, expected_ids):\n",
    "    text = response_text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = text.split(\"\\n\", 1)[1]\n",
    "        text = text.rsplit(\"```\", 1)[0]\n",
    "\n",
    "    try:\n",
    "        results = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            results = ast.literal_eval(text)\n",
    "        except (ValueError, SyntaxError):\n",
    "            match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    results = json.loads(match.group())\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "    if results and isinstance(results[0], list):\n",
    "        results = results[0]\n",
    "\n",
    "    labels = {}\n",
    "    for item in results:\n",
    "        idx = item.get(\"id\")\n",
    "        label = item.get(\"label\", \"\").strip()\n",
    "        conf = item.get(\"confidence\", \"\")\n",
    "        if idx not in expected_ids:\n",
    "            continue\n",
    "        if label not in VALID_LABELS:\n",
    "            matched = [v for v in VALID_LABELS if v.lower() == label.lower()]\n",
    "            if matched:\n",
    "                label = matched[0]\n",
    "            else:\n",
    "                continue\n",
    "        labels[idx] = {\"label\": label, \"confidence\": conf}\n",
    "    return labels\n",
    "\n",
    "\n",
    "def save_results(df):\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        existing = pd.read_csv(OUTPUT_CSV)\n",
    "        combined = pd.concat([existing, df], ignore_index=True)\n",
    "    else:\n",
    "        combined = df\n",
    "    combined.to_csv(OUTPUT_CSV, index=False)\n",
    "    return combined\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29967 comments to label\n",
      "600 requests to submit\n",
      "  Chunk 0: submitted 100 requests -> batches/keh46cnqeyjuktw5xu4gtr904g1qu7owo5b5\n",
      "  Chunk 1: submitted 100 requests -> batches/y8aors0wnhldx115ohdt0s48gwin7dsfpfrq\n",
      "  Chunk 2: submitted 100 requests -> batches/9uouipfoblrwb4muh28cdadfh490nre1qnhg\n",
      "  Chunk 3: submitted 100 requests -> batches/prlvcnjl4y95r2yvpvs5lxbvuj7rcr87k7yg\n",
      "  Chunk 4: submitted 100 requests -> batches/gzss7lkccrx40bpzul86sdiwjdwls4kuw21o\n",
      "  Chunk 5: submitted 100 requests -> batches/8ocypzlq34ogy680e23wrg9amzc1alc29735\n",
      "\n",
      "6 batch jobs submitted\n",
      "Saved to gemini_job_names.json\n",
      "You can close your computer now.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Build inline requests and submit batch jobs\n",
    "#         (100 requests per job due to tier limit)\n",
    "# ============================================================\n",
    "\n",
    "df = subset.copy()\n",
    "\n",
    "# Build batches of comments\n",
    "unlabeled_mask = df[\"label\"].isna()\n",
    "unlabeled_indices = df[unlabeled_mask].index.tolist()\n",
    "print(f\"{len(unlabeled_indices)} comments to label\")\n",
    "\n",
    "batches = []\n",
    "for i in range(0, len(unlabeled_indices), BATCH_SIZE):\n",
    "    batch_indices = unlabeled_indices[i:i + BATCH_SIZE]\n",
    "    batch = [(idx, str(df.loc[idx, COMMENT_COLUMN])) for idx in batch_indices]\n",
    "    batches.append(batch)\n",
    "\n",
    "print(f\"{len(batches)} requests to submit\")\n",
    "\n",
    "# Build inline requests\n",
    "all_requests = []\n",
    "batch_mapping = {}\n",
    "for i, batch in enumerate(batches):\n",
    "    expected_ids = [idx for idx, _ in batch]\n",
    "    batch_mapping[str(i)] = expected_ids\n",
    "    all_requests.append({\n",
    "        'contents': [{\n",
    "            'parts': [{'text': format_batch(batch)}],\n",
    "            'role': 'user'\n",
    "        }],\n",
    "        'config': {\n",
    "            'system_instruction': {'parts': [{'text': SYSTEM_PROMPT}]},\n",
    "            'thinking_config': {'thinking_level': 'minimal'}\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Submit in groups of MAX_REQUESTS_PER_JOB\n",
    "job_names = []\n",
    "for chunk_start in range(0, len(all_requests), MAX_REQUESTS_PER_JOB):\n",
    "    chunk = all_requests[chunk_start:chunk_start + MAX_REQUESTS_PER_JOB]\n",
    "    chunk_num = chunk_start // MAX_REQUESTS_PER_JOB\n",
    "\n",
    "    batch_job = client.batches.create(\n",
    "        model=MODEL,\n",
    "        src=chunk,\n",
    "        config={'display_name': f'labeling-chunk-{chunk_num}'}\n",
    "    )\n",
    "    job_names.append(batch_job.name)\n",
    "    print(f\"  Chunk {chunk_num}: submitted {len(chunk)} requests -> {batch_job.name}\")\n",
    "\n",
    "# Save job names and mapping for retrieval later\n",
    "with open(JOB_NAMES_FILE, \"w\") as f:\n",
    "    json.dump({\"job_names\": job_names, \"batch_mapping\": batch_mapping}, f)\n",
    "\n",
    "print(f\"\\n{len(job_names)} batch jobs submitted\")\n",
    "print(f\"Saved to {JOB_NAMES_FILE}\")\n",
    "print(\"You can close your computer now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches/keh46cnqeyjuktw5xu4gtr904g1qu7owo5b5: JOB_STATE_PENDING\n",
      "batches/y8aors0wnhldx115ohdt0s48gwin7dsfpfrq: JOB_STATE_PENDING\n",
      "batches/9uouipfoblrwb4muh28cdadfh490nre1qnhg: JOB_STATE_PENDING\n",
      "batches/prlvcnjl4y95r2yvpvs5lxbvuj7rcr87k7yg: JOB_STATE_PENDING\n",
      "batches/gzss7lkccrx40bpzul86sdiwjdwls4kuw21o: JOB_STATE_PENDING\n",
      "batches/8ocypzlq34ogy680e23wrg9amzc1alc29735: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Check batch status (run when you come back)\n",
    "# ============================================================\n",
    "\n",
    "with open(JOB_NAMES_FILE, \"r\") as f:\n",
    "    saved = json.load(f)\n",
    "    job_names = saved[\"job_names\"]\n",
    "\n",
    "for name in job_names:\n",
    "    job = client.batches.get(name=name)\n",
    "    print(f\"{job.name}: {job.state.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 2.5: Check for errors in completed jobs\n",
    "# ============================================================\n",
    "\n",
    "with open(JOB_NAMES_FILE, \"r\") as f:\n",
    "    saved = json.load(f)\n",
    "    job_names = saved[\"job_names\"]\n",
    "\n",
    "for name in job_names:\n",
    "    job = client.batches.get(name=name)\n",
    "\n",
    "    if job.state.name == 'JOB_STATE_FAILED':\n",
    "        print(f\"{name}: FAILED\")\n",
    "        if hasattr(job, 'error') and job.error:\n",
    "            print(f\"  Error: {job.error}\")\n",
    "        continue\n",
    "\n",
    "    if job.state.name != 'JOB_STATE_SUCCEEDED':\n",
    "        print(f\"{name}: {job.state.name} (still running)\")\n",
    "        continue\n",
    "\n",
    "    # Check individual request errors within succeeded jobs\n",
    "    error_count = 0\n",
    "    for i, inline_response in enumerate(job.dest.inlined_responses):\n",
    "        if inline_response.error:\n",
    "            error_count += 1\n",
    "            if error_count <= 5:\n",
    "                print(f\"  {name} request {i}: {inline_response.error}\")\n",
    "\n",
    "    if error_count > 5:\n",
    "        print(f\"  ... and {error_count - 5} more errors\")\n",
    "    elif error_count == 0:\n",
    "        print(f\"{name}: all requests succeeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Retrieve results and save\n",
    "#         (run once all jobs show JOB_STATE_SUCCEEDED)\n",
    "# ============================================================\n",
    "\n",
    "with open(JOB_NAMES_FILE, \"r\") as f:\n",
    "    saved = json.load(f)\n",
    "    job_names = saved[\"job_names\"]\n",
    "    batch_mapping = saved[\"batch_mapping\"]\n",
    "\n",
    "df = subset.copy()\n",
    "total_labeled = 0\n",
    "failed = 0\n",
    "request_idx = 0\n",
    "\n",
    "for name in job_names:\n",
    "    job = client.batches.get(name=name)\n",
    "\n",
    "    if job.state.name != 'JOB_STATE_SUCCEEDED':\n",
    "        print(f\"  Skipping {name} — state: {job.state.name}\")\n",
    "        # Count how many requests were in this chunk to keep index aligned\n",
    "        chunk_size = min(MAX_REQUESTS_PER_JOB, len(batch_mapping) - request_idx)\n",
    "        request_idx += chunk_size\n",
    "        continue\n",
    "\n",
    "    for inline_response in job.dest.inlined_responses:\n",
    "        expected_ids = batch_mapping[str(request_idx)]\n",
    "\n",
    "        if inline_response.response:\n",
    "            response_text = inline_response.response.text\n",
    "            labels = parse_response(response_text, expected_ids)\n",
    "\n",
    "            for idx, value in labels.items():\n",
    "                df.loc[idx, \"label\"] = value[\"label\"]\n",
    "                df.loc[idx, \"confidence\"] = value[\"confidence\"]\n",
    "\n",
    "            total_labeled += len(labels)\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  Request {request_idx} failed: {inline_response.error}\")\n",
    "\n",
    "        request_idx += 1\n",
    "\n",
    "combined = save_results(df)\n",
    "\n",
    "print(f\"\\nDONE \\u2014 {total_labeled} comments labeled, {failed} requests failed\")\n",
    "print(f\"Saved to: {OUTPUT_CSV} ({len(combined)} total rows)\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[\"label\"].value_counts().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS-509_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
