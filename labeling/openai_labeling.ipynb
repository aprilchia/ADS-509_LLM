{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_comments = pd.read_csv(\"../data/full_comments.csv\")\n",
    "examples = full_comments.loc[full_comments['true_label'].notna(), [\"comment_text\", \"true_label\"]].sample(10)\n",
    "negative_examples = pd.read_excel(\"negative_examples.xlsx\", names=['comment_text', 'original_label', 'corrected_label'])\n",
    "\n",
    "examples = examples.rename(columns={\"comment_text\": \"comment\", \"true_label\": \"label\"})\n",
    "\n",
    "examples_json = json.dumps(examples.to_dict(orient=\"records\"), indent=2)\n",
    "negative_examples_json = json.dumps(negative_examples.to_dict(orient=\"records\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = full_comments.sample(30000, random_state=10)\n",
    "\n",
    "indices = [a for a in full_comments.index if a not in subset.index]\n",
    "\n",
    "subset = full_comments.loc[indices, :]\n",
    "\n",
    "subset['label'] = subset['true_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "half = int(subset.shape[0] / 2)\n",
    "subset1 = subset[:half]\n",
    "subset2 = subset[half:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMENT_COLUMN = \"comment_text\"\n",
    "OUTPUT_CSV = \"openai_comments_labeled_p2.csv\"\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================\n",
    "\n",
    "MODEL = \"gpt-5-mini\"\n",
    "BATCH_SIZE = 30\n",
    "BATCH_ID_FILE = \"openai_batch_id.txt\"\n",
    "BATCH_INPUT_FILE = \"openai_batch_input.jsonl\"\n",
    "\n",
    "# ============================================================\n",
    "# SYSTEM PROMPT\n",
    "# ============================================================\n",
    "SYSTEM_PROMPT = f\"\"\"You are a comment classifier. You will be given a batch of comments, each with an ID number. \n",
    "Classify each comment into exactly ONE of these five categories:\n",
    "\n",
    "**Argumentative**\n",
    "- Makes specific claims, predictions, or assertions supported by reasoning\n",
    "- Uses evidence, anecdotes, or scenarios to build a case\n",
    "- The key distinction from Opinion: there's an attempt to *persuade* or *explain why*, not just state a position\n",
    "\n",
    "**Informational**\n",
    "- Shares facts, data, links, or context relevant to the discussion\n",
    "- Low emotional affect — the comment is trying to *inform*, not convince or react\n",
    "- Includes answering another commenter's question with factual content\n",
    "- The key distinction from Argumentative: presenting information without advocating for a position\n",
    "\n",
    "**Opinion**\n",
    "- States a value judgment, stance, or take without substantial reasoning\n",
    "- \"This is good/bad/wrong/overrated\" — the comment *asserts* but doesn't *argue*\n",
    "- The key distinction from Argumentative: no real attempt to persuade or support the claim\n",
    "- The key distinction from Expressive: the comment is making a point, not just reacting\n",
    "\n",
    "**Expressive**\n",
    "- Emotional reactions, sarcasm, jokes, venting, exclamations\n",
    "- The comment is primarily *expressing feeling* rather than making a point\n",
    "- Includes performative agreement/disagreement (\"THIS,\" \"lol exactly,\" \"what a joke\")\n",
    "- The key distinction from Opinion: no identifiable stance being taken, just affect\n",
    "\n",
    "**Neutral**\n",
    "- Clarifying or rhetorical questions, meta-commentary, off-topic remarks\n",
    "- Comments that don't clearly fit the other four categories\n",
    "- Includes simple factual questions directed at other commenters\n",
    "\n",
    "**Correctly labeled examples** — these demonstrate the correct label for each comment:\n",
    "{examples_json}\n",
    "\n",
    "**Incorrectly labeled examples** — these were originally mislabeled. The \"original_label\" is the wrong label that was assigned, and the \"corrected_label\" is what the label should have been. Use these to understand common mistakes to avoid:\n",
    "{negative_examples_json}\n",
    "\n",
    "Respond with ONLY a valid JSON array where each element has \"id\", \"label\" keys and a confidence indicator where \n",
    "0 is not confident in the chosen label and 1 is confident in the chosen label.\n",
    "Example: [{{\"id\": 0, \"label\": \"Argumentative\", \"confidence\": 1}}, {{\"id\": 1, \"label\": \"Expressive\", \"confidence\": 0}}]\n",
    "\n",
    "Do not include any text outside the JSON array. No explanations, no markdown.\"\"\"\n",
    "\n",
    "VALID_LABELS = {\"Argumentative\", \"Informational\", \"Opinion\", \"Expressive\", \"Neutral\"}\n",
    "\n",
    "\n",
    "def format_batch(comments):\n",
    "    lines = []\n",
    "    for idx, comment in comments:\n",
    "        truncated = comment[:1500] if len(comment) > 1500 else comment\n",
    "        lines.append(f\"[{idx}] {truncated}\")\n",
    "    return \"\\n\\n\".join(lines)\n",
    "\n",
    "\n",
    "def parse_response(response_text, expected_ids):\n",
    "    text = response_text.strip()\n",
    "    if text.startswith(\"```\"):\n",
    "        text = text.split(\"\\n\", 1)[1]\n",
    "        text = text.rsplit(\"```\", 1)[0]\n",
    "\n",
    "    try:\n",
    "        results = json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        try:\n",
    "            results = ast.literal_eval(text)\n",
    "        except (ValueError, SyntaxError):\n",
    "            match = re.search(r'\\[.*\\]', text, re.DOTALL)\n",
    "            if match:\n",
    "                try:\n",
    "                    results = json.loads(match.group())\n",
    "                except json.JSONDecodeError:\n",
    "                    return {}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "    if results and isinstance(results[0], list):\n",
    "        results = results[0]\n",
    "\n",
    "    labels = {}\n",
    "    for item in results:\n",
    "        idx = item.get(\"id\")\n",
    "        label = item.get(\"label\", \"\").strip()\n",
    "        conf = item.get(\"confidence\", \"\")\n",
    "        if idx not in expected_ids:\n",
    "            continue\n",
    "        if label not in VALID_LABELS:\n",
    "            matched = [v for v in VALID_LABELS if v.lower() == label.lower()]\n",
    "            if matched:\n",
    "                label = matched[0]\n",
    "            else:\n",
    "                continue\n",
    "        labels[idx] = {\"label\": label, \"confidence\": conf}\n",
    "    return labels\n",
    "\n",
    "\n",
    "def save_results(df):\n",
    "    if os.path.exists(OUTPUT_CSV):\n",
    "        existing = pd.read_csv(OUTPUT_CSV)\n",
    "        combined = pd.concat([existing, df], ignore_index=True)\n",
    "    else:\n",
    "        combined = df\n",
    "    combined.to_csv(OUTPUT_CSV, index=False)\n",
    "    return combined\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23819 comments to label\n",
      "794 requests written to openai_batch_input.jsonl\n",
      "File uploaded: file-XtQGENXQWXnLmhFhNFsNeh\n",
      "Batch submitted! ID: batch_699376e36e388190b6b2f3ec5ea2f993\n",
      "Saved batch_id to openai_batch_id.txt and mapping to openai_batch_mapping.json\n",
      "You can close your computer now.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 1: Build .jsonl file, upload, and submit batch\n",
    "# ============================================================\n",
    "\n",
    "df = subset2.copy()\n",
    "\n",
    "# Build batches of comments\n",
    "unlabeled_mask = df[\"label\"].isna()\n",
    "unlabeled_indices = df[unlabeled_mask].index.tolist()\n",
    "print(f\"{len(unlabeled_indices)} comments to label\")\n",
    "\n",
    "batches = []\n",
    "for i in range(0, len(unlabeled_indices), BATCH_SIZE):\n",
    "    batch_indices = unlabeled_indices[i:i + BATCH_SIZE]\n",
    "    batch = [(idx, str(df.loc[idx, COMMENT_COLUMN])) for idx in batch_indices]\n",
    "    batches.append(batch)\n",
    "\n",
    "# Save mapping of batch index -> expected IDs for retrieval later\n",
    "batch_mapping = {}\n",
    "\n",
    "# Write .jsonl file\n",
    "with open(BATCH_INPUT_FILE, \"w\") as f:\n",
    "    for i, batch in enumerate(batches):\n",
    "        expected_ids = [idx for idx, _ in batch]\n",
    "        batch_mapping[str(i)] = expected_ids\n",
    "        request = {\n",
    "            \"custom_id\": f\"batch_{i}\",\n",
    "            \"method\": \"POST\",\n",
    "            \"url\": \"/v1/chat/completions\",\n",
    "            \"body\": {\n",
    "                \"model\": MODEL,\n",
    "                \"max_completion_tokens\": 8092,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": format_batch(batch)}\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        f.write(json.dumps(request) + \"\\n\")\n",
    "\n",
    "print(f\"{len(batches)} requests written to {BATCH_INPUT_FILE}\")\n",
    "\n",
    "# Upload the file\n",
    "batch_file = client.files.create(\n",
    "    file=open(BATCH_INPUT_FILE, \"rb\"),\n",
    "    purpose=\"batch\"\n",
    ")\n",
    "print(f\"File uploaded: {batch_file.id}\")\n",
    "\n",
    "# Submit the batch\n",
    "batch_job = client.batches.create(\n",
    "    input_file_id=batch_file.id,\n",
    "    endpoint=\"/v1/chat/completions\",\n",
    "    completion_window=\"24h\"\n",
    ")\n",
    "\n",
    "# Save batch_id and mapping for retrieval later\n",
    "with open(BATCH_ID_FILE, \"w\") as f:\n",
    "    f.write(batch_job.id)\n",
    "\n",
    "with open(\"openai_batch_mapping.json\", \"w\") as f:\n",
    "    json.dump(batch_mapping, f)\n",
    "\n",
    "print(f\"Batch submitted! ID: {batch_job.id}\")\n",
    "print(f\"Saved batch_id to {BATCH_ID_FILE} and mapping to openai_batch_mapping.json\")\n",
    "print(\"You can close your computer now.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: in_progress\n",
      "Counts: BatchRequestCounts(completed=778, failed=0, total=794)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 2: Check batch status (run when you come back)\n",
    "# ============================================================\n",
    "\n",
    "with open(BATCH_ID_FILE, \"r\") as f:\n",
    "    batch_id = f.read().strip()\n",
    "\n",
    "status = client.batches.retrieve(batch_id)\n",
    "print(f\"Status: {status.status}\")\n",
    "print(f\"Counts: {status.request_counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check error details\n",
    "batch_job = client.batches.retrieve(batch_id)\n",
    "if batch_job.error_file_id:\n",
    "    error_content = client.files.content(batch_job.error_file_id).content\n",
    "    with open(\"openai_batch_errors.jsonl\", \"wb\") as f:\n",
    "        f.write(error_content)\n",
    "\n",
    "    with open(\"openai_batch_errors.jsonl\", \"r\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            err = json.loads(line.strip())\n",
    "            print(f\"{err['custom_id']}: {err['error']}\")\n",
    "            if i >= 4:\n",
    "                print(\"...\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results downloaded to openai_batch_results.jsonl\n",
      "\n",
      "DONE — 23827 comments labeled, 0 batches failed\n",
      "Saved to: openai_comments_labeled_p1.csv (23857 total rows)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "Argumentative    8287\n",
      "Expressive       5317\n",
      "Opinion          4908\n",
      "Informational    3196\n",
      "Neutral          2146\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# STEP 3: Retrieve results and save (run once status is \"completed\")\n",
    "# ============================================================\n",
    "\n",
    "with open(BATCH_ID_FILE, \"r\") as f:\n",
    "    batch_id = f.read().strip()\n",
    "\n",
    "with open(\"openai_batch_mapping.json\", \"r\") as f:\n",
    "    batch_mapping = json.load(f)\n",
    "\n",
    "# Download results file\n",
    "batch_job = client.batches.retrieve(batch_id)\n",
    "result_content = client.files.content(batch_job.output_file_id).content\n",
    "\n",
    "result_file = \"openai_batch_results.jsonl\"\n",
    "with open(result_file, \"wb\") as f:\n",
    "    f.write(result_content)\n",
    "\n",
    "print(f\"Results downloaded to {result_file}\")\n",
    "\n",
    "# Parse results\n",
    "df = subset2.copy()\n",
    "total_labeled = 0\n",
    "failed = 0\n",
    "\n",
    "with open(result_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        result = json.loads(line.strip())\n",
    "        batch_idx = result[\"custom_id\"].split(\"_\")[1]\n",
    "        expected_ids = batch_mapping[batch_idx]\n",
    "\n",
    "        if result[\"error\"] is None:\n",
    "            response_text = result[\"response\"][\"body\"][\"choices\"][0][\"message\"][\"content\"]\n",
    "            labels = parse_response(response_text, expected_ids)\n",
    "\n",
    "            for idx, value in labels.items():\n",
    "                df.loc[idx, \"label\"] = value[\"label\"]\n",
    "                df.loc[idx, \"confidence\"] = value[\"confidence\"]\n",
    "\n",
    "            total_labeled += len(labels)\n",
    "        else:\n",
    "            failed += 1\n",
    "            print(f\"  Batch {batch_idx} failed: {result['error']}\")\n",
    "\n",
    "combined = save_results(df)\n",
    "\n",
    "print(f\"\\nDONE \\u2014 {total_labeled} comments labeled, {failed} batches failed\")\n",
    "print(f\"Saved to: {OUTPUT_CSV} ({len(combined)} total rows)\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df[\"label\"].value_counts().to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS-509_LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
