{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f26c9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "os.environ[\"TENSORBOARD_LOGGING_DIR\"] = \"./logs\"\n",
    "\n",
    "MODEL_ID = 'vinai/bertweet-large'\n",
    "\n",
    "# check for gpu\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27aeab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use your hugging face token to log in\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a4628fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1acbae4d47748f8a2cf25a628db78f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/895 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030aab7466d545e98dceae0f8ee03639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/3.69M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63341fa5481a4bc1842cc3d85b9c2928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/819k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf4ad725d7642c9aec7603c6f0cd658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/valid-00000-of-00001.parquet:   0%|          | 0.00/794k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212a2933d9c040c98a322b93bb9187de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/23949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ad1ff559bc4420a7ddb664b20a54bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5133 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03ef724cd9364842a3b32aa2fcec1ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/5132 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset you're using, don't forget to specify the data directory\n",
    "dataset = load_dataset(\"ADS509/experiment_labels_full_match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eaa5a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {}\n",
    "id2label = {}\n",
    "\n",
    "for i, label in enumerate(dataset['train'].features['label'].names):\n",
    "\n",
    "    label2id[label] = i\n",
    "    id2label[i] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7062c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Function to tokenize data with\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(\n",
    "        batch['text'],\n",
    "        truncation=True, \n",
    "        #padding='max_length',\n",
    "        max_length=512 # Can't be greater than model max length\n",
    "    )\n",
    "# Data collator handles padding dynamically, set padding and max_length if you want to control it explicitly and drop the collator\n",
    "\n",
    "# Tokenize Data\n",
    "train_data = dataset['train'].map(tokenize_function, batched=True)\n",
    "test_data = dataset['test'].map(tokenize_function, batched=True)\n",
    "valid_data = dataset['valid'].map(tokenize_function, batched=True)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_data.set_format(\"torch\", columns=['input_ids', \"attention_mask\", \"label\"])\n",
    "test_data.set_format(\"torch\", columns=['input_ids', \"attention_mask\", \"label\"])\n",
    "valid_data.set_format(\"torch\", columns=['input_ids', \"attention_mask\", \"label\"])\n",
    "\n",
    "    \n",
    "# Verify batch\n",
    "test_loader = DataLoader(train_data, batch_size=4)\n",
    "batch = next(iter(test_loader))\n",
    "print(f\"Batch keys: {batch.keys()}\")\n",
    "print(f\"Input IDs shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Labels shape: {batch['label'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7dbede08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a42570e1e144c9bdfb2e5675a62eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RobertaForSequenceClassification LOAD REPORT from: vinai/bertweet-large\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.decoder.bias            | UNEXPECTED | \n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.decoder.weight          | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "classifier.out_proj.bias        | MISSING    | \n",
      "classifier.out_proj.weight      | MISSING    | \n",
      "classifier.dense.bias           | MISSING    | \n",
      "classifier.dense.weight         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "# Pick a repo name to save the trained model to\n",
    "# model_repo = \"experiment_labels_bert_base\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=5, # adjust this based on number of labels you're training on\n",
    "    device_map='cuda',\n",
    "    dtype='auto',\n",
    "    label2id=label2id, # set these two args to attach the metadata to the model.config\n",
    "    id2label=id2label\n",
    ")\n",
    "\n",
    "# Metric function for evaluation in Trainer\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted')\n",
    "    }\n",
    "\n",
    "# Data collator to handle padding dynamically per batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-comment', # Saves it locally\n",
    "    #push_to_hub=True,\n",
    "    #hub_model_id=f\"ADS509/{model_repo}\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,  # or warmup_ratio=%\n",
    "    \n",
    "    # Evaluation & saving\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "    \n",
    "    # Logging\n",
    "    logging_steps=100,\n",
    "    report_to='tensorboard',\n",
    "    \n",
    "    # Other\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),  # Mixed precision if GPU available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2f2c800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1498' max='1498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1498/1498 05:59, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.383654</td>\n",
       "      <td>0.353443</td>\n",
       "      <td>0.874513</td>\n",
       "      <td>0.863622</td>\n",
       "      <td>0.875051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.216188</td>\n",
       "      <td>0.310921</td>\n",
       "      <td>0.892050</td>\n",
       "      <td>0.886037</td>\n",
       "      <td>0.892004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d804eb69ce654c728dcfb55ce8f6c457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240f4c91a3864a5fb13fe43d931bb9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.12.attention.output.LayerNorm.weight', 'roberta.encoder.layer.12.attention.output.LayerNorm.bias', 'roberta.encoder.layer.12.output.LayerNorm.weight', 'roberta.encoder.layer.12.output.LayerNorm.bias', 'roberta.encoder.layer.13.attention.output.LayerNorm.weight', 'roberta.encoder.layer.13.attention.output.LayerNorm.bias', 'roberta.encoder.layer.13.output.LayerNorm.weight', 'roberta.encoder.layer.13.output.LayerNorm.bias', 'roberta.encoder.layer.14.attention.output.LayerNorm.weight', 'roberta.encoder.layer.14.attention.output.LayerNorm.bias', 'roberta.encoder.layer.14.output.LayerNorm.weight', 'roberta.encoder.layer.14.output.LayerNorm.bias', 'roberta.encoder.layer.15.attention.output.LayerNorm.weight', 'roberta.encoder.layer.15.attention.output.LayerNorm.bias', 'roberta.encoder.layer.15.output.LayerNorm.weight', 'roberta.encoder.layer.15.output.LayerNorm.bias', 'roberta.encoder.layer.16.attention.output.LayerNorm.weight', 'roberta.encoder.layer.16.attention.output.LayerNorm.bias', 'roberta.encoder.layer.16.output.LayerNorm.weight', 'roberta.encoder.layer.16.output.LayerNorm.bias', 'roberta.encoder.layer.17.attention.output.LayerNorm.weight', 'roberta.encoder.layer.17.attention.output.LayerNorm.bias', 'roberta.encoder.layer.17.output.LayerNorm.weight', 'roberta.encoder.layer.17.output.LayerNorm.bias', 'roberta.encoder.layer.18.attention.output.LayerNorm.weight', 'roberta.encoder.layer.18.attention.output.LayerNorm.bias', 'roberta.encoder.layer.18.output.LayerNorm.weight', 'roberta.encoder.layer.18.output.LayerNorm.bias', 'roberta.encoder.layer.19.attention.output.LayerNorm.weight', 'roberta.encoder.layer.19.attention.output.LayerNorm.bias', 'roberta.encoder.layer.19.output.LayerNorm.weight', 'roberta.encoder.layer.19.output.LayerNorm.bias', 'roberta.encoder.layer.20.attention.output.LayerNorm.weight', 'roberta.encoder.layer.20.attention.output.LayerNorm.bias', 'roberta.encoder.layer.20.output.LayerNorm.weight', 'roberta.encoder.layer.20.output.LayerNorm.bias', 'roberta.encoder.layer.21.attention.output.LayerNorm.weight', 'roberta.encoder.layer.21.attention.output.LayerNorm.bias', 'roberta.encoder.layer.21.output.LayerNorm.weight', 'roberta.encoder.layer.21.output.LayerNorm.bias', 'roberta.encoder.layer.22.attention.output.LayerNorm.weight', 'roberta.encoder.layer.22.attention.output.LayerNorm.bias', 'roberta.encoder.layer.22.output.LayerNorm.weight', 'roberta.encoder.layer.22.output.LayerNorm.bias', 'roberta.encoder.layer.23.attention.output.LayerNorm.weight', 'roberta.encoder.layer.23.attention.output.LayerNorm.bias', 'roberta.encoder.layer.23.output.LayerNorm.weight', 'roberta.encoder.layer.23.output.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['roberta.embeddings.LayerNorm.beta', 'roberta.embeddings.LayerNorm.gamma', 'roberta.encoder.layer.0.attention.output.LayerNorm.beta', 'roberta.encoder.layer.0.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.0.output.LayerNorm.beta', 'roberta.encoder.layer.0.output.LayerNorm.gamma', 'roberta.encoder.layer.1.attention.output.LayerNorm.beta', 'roberta.encoder.layer.1.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.1.output.LayerNorm.beta', 'roberta.encoder.layer.1.output.LayerNorm.gamma', 'roberta.encoder.layer.2.attention.output.LayerNorm.beta', 'roberta.encoder.layer.2.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.2.output.LayerNorm.beta', 'roberta.encoder.layer.2.output.LayerNorm.gamma', 'roberta.encoder.layer.3.attention.output.LayerNorm.beta', 'roberta.encoder.layer.3.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.3.output.LayerNorm.beta', 'roberta.encoder.layer.3.output.LayerNorm.gamma', 'roberta.encoder.layer.4.attention.output.LayerNorm.beta', 'roberta.encoder.layer.4.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.4.output.LayerNorm.beta', 'roberta.encoder.layer.4.output.LayerNorm.gamma', 'roberta.encoder.layer.5.attention.output.LayerNorm.beta', 'roberta.encoder.layer.5.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.5.output.LayerNorm.beta', 'roberta.encoder.layer.5.output.LayerNorm.gamma', 'roberta.encoder.layer.6.attention.output.LayerNorm.beta', 'roberta.encoder.layer.6.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.6.output.LayerNorm.beta', 'roberta.encoder.layer.6.output.LayerNorm.gamma', 'roberta.encoder.layer.7.attention.output.LayerNorm.beta', 'roberta.encoder.layer.7.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.7.output.LayerNorm.beta', 'roberta.encoder.layer.7.output.LayerNorm.gamma', 'roberta.encoder.layer.8.attention.output.LayerNorm.beta', 'roberta.encoder.layer.8.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.8.output.LayerNorm.beta', 'roberta.encoder.layer.8.output.LayerNorm.gamma', 'roberta.encoder.layer.9.attention.output.LayerNorm.beta', 'roberta.encoder.layer.9.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.9.output.LayerNorm.beta', 'roberta.encoder.layer.9.output.LayerNorm.gamma', 'roberta.encoder.layer.10.attention.output.LayerNorm.beta', 'roberta.encoder.layer.10.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.10.output.LayerNorm.beta', 'roberta.encoder.layer.10.output.LayerNorm.gamma', 'roberta.encoder.layer.11.attention.output.LayerNorm.beta', 'roberta.encoder.layer.11.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.11.output.LayerNorm.beta', 'roberta.encoder.layer.11.output.LayerNorm.gamma', 'roberta.encoder.layer.12.attention.output.LayerNorm.beta', 'roberta.encoder.layer.12.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.12.output.LayerNorm.beta', 'roberta.encoder.layer.12.output.LayerNorm.gamma', 'roberta.encoder.layer.13.attention.output.LayerNorm.beta', 'roberta.encoder.layer.13.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.13.output.LayerNorm.beta', 'roberta.encoder.layer.13.output.LayerNorm.gamma', 'roberta.encoder.layer.14.attention.output.LayerNorm.beta', 'roberta.encoder.layer.14.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.14.output.LayerNorm.beta', 'roberta.encoder.layer.14.output.LayerNorm.gamma', 'roberta.encoder.layer.15.attention.output.LayerNorm.beta', 'roberta.encoder.layer.15.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.15.output.LayerNorm.beta', 'roberta.encoder.layer.15.output.LayerNorm.gamma', 'roberta.encoder.layer.16.attention.output.LayerNorm.beta', 'roberta.encoder.layer.16.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.16.output.LayerNorm.beta', 'roberta.encoder.layer.16.output.LayerNorm.gamma', 'roberta.encoder.layer.17.attention.output.LayerNorm.beta', 'roberta.encoder.layer.17.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.17.output.LayerNorm.beta', 'roberta.encoder.layer.17.output.LayerNorm.gamma', 'roberta.encoder.layer.18.attention.output.LayerNorm.beta', 'roberta.encoder.layer.18.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.18.output.LayerNorm.beta', 'roberta.encoder.layer.18.output.LayerNorm.gamma', 'roberta.encoder.layer.19.attention.output.LayerNorm.beta', 'roberta.encoder.layer.19.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.19.output.LayerNorm.beta', 'roberta.encoder.layer.19.output.LayerNorm.gamma', 'roberta.encoder.layer.20.attention.output.LayerNorm.beta', 'roberta.encoder.layer.20.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.20.output.LayerNorm.beta', 'roberta.encoder.layer.20.output.LayerNorm.gamma', 'roberta.encoder.layer.21.attention.output.LayerNorm.beta', 'roberta.encoder.layer.21.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.21.output.LayerNorm.beta', 'roberta.encoder.layer.21.output.LayerNorm.gamma', 'roberta.encoder.layer.22.attention.output.LayerNorm.beta', 'roberta.encoder.layer.22.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.22.output.LayerNorm.beta', 'roberta.encoder.layer.22.output.LayerNorm.gamma', 'roberta.encoder.layer.23.attention.output.LayerNorm.beta', 'roberta.encoder.layer.23.attention.output.LayerNorm.gamma', 'roberta.encoder.layer.23.output.LayerNorm.beta', 'roberta.encoder.layer.23.output.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3109213709831238, 'eval_accuracy': 0.892049883086516, 'eval_f1_macro': 0.8860368383380253, 'eval_f1_weighted': 0.8920043912778948, 'eval_runtime': 11.4518, 'eval_samples_per_second': 448.138, 'eval_steps_per_second': 7.073, 'epoch': 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Set up Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train!\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "747b6d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d969c7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "Argumentative       0.76      0.80      0.78      2423\n",
      "   Expressive       0.82      0.77      0.80      3289\n",
      "Informational       0.68      0.68      0.68       810\n",
      "      Neutral       0.69      0.70      0.69       779\n",
      "      Opinion       0.71      0.72      0.71      3257\n",
      "\n",
      "     accuracy                           0.75     10558\n",
      "    macro avg       0.73      0.73      0.73     10558\n",
      " weighted avg       0.75      0.75      0.75     10558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "true_ints = np.asarray(test_data['label'], dtype=int)\n",
    "true_labels = [id2label[i] for i in true_ints]\n",
    "\n",
    "pred_ints = np.argmax(preds.predictions, axis=1)\n",
    "pred_labels = [id2label[i] for i in pred_ints]\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51acf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.args.hub_model_id = \"ADS509/BERTweet-large-full-match-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5fcb60e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a81e793ccb4fd9bf975a9d52a7144d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "207aac1fd3da4e148d05e7084c1ab62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing model shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0741f43da33f48e5864298133054ad77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a3d02d470544fcaa8e12ed97e5f442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d5b34b2d95443019fcb2f43e0154b39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...comment/model.safetensors:   0%|          | 47.8kB / 1.42GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f990b9f927d34c69a575eb54d91d9cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...comment/training_args.bin:  29%|##9       | 1.54kB / 5.26kB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ADS509/BERTweet-large-full-match-data/commit/7d86942249dc908c034391fcc403c862dcfb2acf', commit_message='BERTweet-large with correct labels on full match data', commit_description='', oid='7d86942249dc908c034391fcc403c862dcfb2acf', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ADS509/BERTweet-large-full-match-data', endpoint='https://huggingface.co', repo_type='model', repo_id='ADS509/BERTweet-large-full-match-data'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model to hugging face model repo\n",
    "trainer.save_model(training_args.output_dir)\n",
    "trainer.push_to_hub(commit_message = \"BERTweet-large with correct labels on full match data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96861f00",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf010d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i9pvk2mukb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Trainer's built-in hyperparameter_search with Optuna\n",
    "import optuna\n",
    "import transformers\n",
    "import logging\n",
    "\n",
    "def model_init():\n",
    "    # Temporarily suppress model load reports\n",
    "    transformers.logging.set_verbosity_error()\n",
    "    logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.ERROR)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        num_labels=5,\n",
    "        device_map='cuda',\n",
    "        dtype='auto',\n",
    "    )\n",
    "    transformers.logging.set_verbosity_warning()\n",
    "    logging.getLogger(\"accelerate.utils.modeling\").setLevel(logging.WARNING)\n",
    "    return model\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"num_train_epochs\": trial.suggest_int(\"epochs\", 2, 3),\n",
    "        \"learning_rate\": trial.suggest_float(\"lr\", 1e-5, 1e-4, log=True),\n",
    "        \"warmup_steps\": trial.suggest_int(\"warmup\", 100, 300, step=50),\n",
    "        \"weight_decay\": trial.suggest_float(\"decay\", 0, 0.2, step=0.05),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"batch_size\", [16, 32, 64]),\n",
    "        \"optim\": trial.suggest_categorical(\"optimizer\", [\"adamw_torch\", \"adamw_torch_fused\", \"adafactor\"]),\n",
    "    }\n",
    "\n",
    "search_args = TrainingArguments(\n",
    "    output_dir='./hp-search',\n",
    "    per_device_eval_batch_size=64,\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    metric_for_best_model='f1_macro',\n",
    "    report_to='none',\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "hp_trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=search_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best_run = hp_trainer.hyperparameter_search(\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda metrics: metrics[\"eval_f1_macro\"],\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    n_trials=10,\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=2, n_warmup_steps=1),\n",
    ")\n",
    "\n",
    "print(\"Best run:\")\n",
    "print(f\"  F1 Macro: {best_run.objective:.4f}\")\n",
    "print(f\"  Params: {best_run.hyperparameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9hyi1n3jbnu",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain with best hyperparameters from hyperparameter_search and push to Hub\n",
    "repo_id = \"best_hp_tuning_v1\"\n",
    "\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    num_labels=5,\n",
    "    device_map='cuda',\n",
    "    dtype='auto',\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")\n",
    "\n",
    "best_hp = best_run.hyperparameters\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=f'./best-{repo_id}',\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=f\"ADS509/{repo_id}\",\n",
    "    optim=best_hp['optimizer'],\n",
    "    num_train_epochs=best_hp['epochs'],\n",
    "    per_device_train_batch_size=best_hp['batch_size'],\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=best_hp['lr'],\n",
    "    weight_decay=best_hp['decay'],\n",
    "    warmup_steps=best_hp['warmup'],\n",
    "\n",
    "    eval_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1_macro',\n",
    "\n",
    "    logging_steps=100,\n",
    "    report_to='tensorboard',\n",
    "\n",
    "    seed=42,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "best_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=valid_data,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "best_trainer.train()\n",
    "eval_results = best_trainer.evaluate()\n",
    "print(eval_results)\n",
    "\n",
    "best_trainer.save_model(best_args.output_dir)\n",
    "best_trainer.push_to_hub(commit_message=f\"Best model from HP search (f1_macro={best_run.objective:.4f})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
